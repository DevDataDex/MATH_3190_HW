---
title: "Homework 6"
author: "Devin Warner"
date: '2022-04-11'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(glmnet)
library(psych)
```

In this homework you will practice using cross-validation to fit data using LASSO and K-nearest neighbor models. Please upload to your GitHub an R Markdown document answering the following:   


## Question 1 (20 points) 
A researcher wants to determine how employee salaries at a certain company are related to the length of employment, previous experience, and education. The researcher selects eight employees from the company and obtains the data shown below (the dataset is available as a tibble in the .Rmd). 

```{r q1, echo = FALSE}
salary <- tibble(
    Salary = c(57310.00, 57380.00, 54135.00, 56985.00, 58715.00, 60620.00, 59200.00, 60320.00),
    Employment = c(10,5,3,6,8,20,8,14),
    Experience = c(2,6,1,5,8,0,4,6),
    Education = c(16,16,12,14,16,12,18,17)
)
salary
```

a) Fit a standard least squares regression model to these data and interpret the results. After looking at the statistical significance of the $\beta$s, which covariates would you include in a final model? 

```{r q1_a}
  lm_model <- lm(Salary~.,data = salary)
  summary(lm_model)
```

The standard least squares regression line using Employment, Experience and Education explains about 94% of the variation in salary. Of these variables, Employment is significant at the $\alpha$=0.01 level, while the other two variables are not significant at the $\alpha$=0.10 level. Based off of the statistical significance of the $\beta$s only, I would chose only Employment to include in a final model.

b) Use \texttt{glmnet} to fit a LASSO model to these covariates. Try $\lambda$=1000, 800, 500, and 1. How do the results compare to each other and the least squares model? 

```{r q1_b}
lasso_salary_1000 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 1000,
                            family = "gaussian", alpha = 1)
lasso_salary_1000$beta

lasso_salary_800 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 800,
                           family = "gaussian", alpha = 1)
lasso_salary_800$beta

lasso_salary_500 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 500,
                           family = "gaussian", alpha = 1)
lasso_salary_500$beta

lasso_salary_1 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 1,
                         family = "gaussian", alpha = 1)
lasso_salary_1$beta
```

We see that the LASSO model with $\lambda = 1$ has model coefficients that equivalent to the standard least squares regression line. As lambda increases, the coefficients on Experience and Education decrease until they are equivalent to 0 in the model. The coefficient on Experience goes to 0 first, and the Education. This agrees with my choice in part (a) to only include Employment in the final model. 

c) Which LASSO model (i.e. $\lambda$) would you select? (note you are not just restricted to $\lambda$ values of 1000, 800, 500, and 1). Justify your answer. 

We want to select the $\lambda$ value that produces the best accuracy for prediction. Using cross validation we can find the optimal $\lambda$ value. 

```{r q1_c}
lasso.cv <- cv.glmnet(as.matrix(salary[,-1]), salary$Salary, alpha = 1)
plot(lasso.cv)
(lasso.lambda <- lasso.cv$lambda.min)

```


As shown above, the best $\lambda$ value for prediction is 7.351223.

d) Use \texttt{glmnet} to fit a Ridge regression model to these data. Try $\lambda$=1000, 800, 500, and 1. How do these results differ from the least squares and LASSO models? 

```{r q1_d}
ridge_salary_1000 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 1000,
                            family = "gaussian", alpha = 0)
ridge_salary_1000$beta

ridge_salary_800 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 800,
                           family = "gaussian", alpha = 0)
ridge_salary_800$beta

ridge_salary_500 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 500,
                           family = "gaussian", alpha = 0)
ridge_salary_500$beta

ridge_salary_1 <- glmnet(as.matrix(salary[,-1]), salary$Salary, lambda = 1,
                         family = "gaussian", alpha = 0)
ridge_salary_1$beta
```

Again, a Ridge model with $\lambda = 1$ is equivalent to the standard least square regression line. We see in the Ridge model though that variable coefficients do not decrease to 0 as $\lambda$ increases. However, they still decrease, with the coefficient on Experience decreasing first and by a larger magnitude than Education decreases. 

## Question 2 (20 points)

The \texttt{cereal.csv} dataset provides nutritional information on nearly 80 common breakfast cereals. The 'rating' column provides an overall rating for each cereal (possibly from Consumer Reports?). Use a LASSO regression model to identify the best predictors of cereal rating. Evaluate the model for $\lambda$ values of 8, 5, 3, and 1 (among others). Which $\lambda$ would you choose and why? Which covariates best explain the rating? 

```{r q2_Read/Clean, echo = FALSE, include = FALSE}
cereal <- read_csv("cereal.csv") %>%
  mutate(mfrA = ifelse(mfr == 'A', 1,0)) %>%
  mutate(mfrG = ifelse(mfr == 'G', 1,0)) %>%
  mutate(mfrK = ifelse(mfr == 'K', 1,0)) %>%
  mutate(mfrN = ifelse(mfr == 'N', 1,0)) %>%
  mutate(mfrP = ifelse(mfr == 'P', 1,0)) %>%
  mutate(mfrQ = ifelse(mfr == 'Q', 1,0)) %>%
  mutate(mfrR = ifelse(mfr == 'R', 1,0)) %>%
  mutate(typeCold = ifelse(type == 'C', 1,0)) %>%
  mutate(typeHot = ifelse(type == 'H', 1,0)) %>%
  select(-c(mfr, type))
```

We will use a cross validation to determine the best value of $\lambda$ for our data. First, we will split our data into training and testing data. (For reproducibility's sake, we will set the seed to 8561)

```{r q2_split}
set.seed(8561)

test_ind <- sample(nrow(cereal), 0.3*nrow(cereal))
cereal_test <- cereal[test_ind,]
cereal_train <- cereal[-test_ind,]
```

Next, we will train a cross validation LASSO regression model using our training data. We will exclude the *name* covariate from our modeling as it is an identifying factor. Output below is the change in change in MSE as $\lambda$ increases, as well as the $\lambda$ value that gives the lowest (best) MSE. 

```{r q2_lasso.cv}
cereal_lasso <- cv.glmnet(as.matrix(cereal_train[,-c(1,14)]), cereal_train$rating, alpha = 1, lamdba = seq(1,10))
plot(cereal_lasso)
(cereal.lambda <- cereal_lasso$lambda.min)
```

Now, we will output the coefficients of the LASSO model with our best $\lambda$ value, as well as the MSE for the training data. 

```{r coeff, echo = FALSE}
predict(cereal_lasso,s=cereal.lambda,type="coefficients")

train_preds <- predict(cereal_lasso,s=cereal.lambda,newx = as.matrix(cereal_train[,-c(1,14)]))
train_mse <- mean((train_preds-cereal_train$rating)^2)
print(paste("The training MSE with \u03BB =", round(cereal.lambda,2), "is:", round(train_mse,6)))
```

The variables (covariates) that have non-zero coefficients are the best for explaining rating. Since the coefficients are penalized, the larger the coefficient is, the better it is at explaining rating. We see that the covariates *fiber* and *protein* have the greatest positive affect on rating, and *fat* and *sugars* have the greatest negative affect on rating. 

It is important to note that the *mfr* variables that were excluded from the model appeared very sparsely in the data set (less than 10 observations). The same happens with the *type* variable as there are only 3 *H* observations. This could potentially be a problem because none of these observations may have ended up in the training data because they were so sparse.

Now, we will see how our LASSO model with the best $\lambda$ value does at predicting unseen data. This model has 14 covariates. For reference, we will calculate a baseline accuracy of the test data set using the mean of *rating* as the estimate for all observations. 

```{r q2_preds}
baseline <- mean((mean(cereal_test$rating)-cereal_test$rating)^2)
  
cereal_preds <- predict(cereal_lasso, s = cereal.lambda, newx = as.matrix(cereal_test[,-c(1,14)]))
mse <- mean((cereal_preds - cereal_test$rating)^2)
```

```{r q2_print, echo = FALSE}
print(paste("The baseline MSE is:", baseline))
print(paste("The LASSO model with \u03BB =", round(cereal.lambda,2), " MSE is:", round(mse,6)))
```

Okay, we are seeing a huge improvement in accuracy from the baseline. At first glance, this can be really awesome! Our model didn't do as well on the testing data as the training data, which is to be expected, but not different enough to suggest overfitting. A potential issue is that our model has 14 covariates and the test data only has 13 observations. This shouldn't be an issue though, because those 13 points were unseen while training the model. 

## Question 3 (20 points)

An automobile consulting company wants to understand the factors on which the pricing of cars depends. Use an Elastic Net model and the \texttt{car\_price\_prediction.csv} dataset to determine which variables are significant in predicting the price of a car. Use cross-validation to find an optimal value for $\lambda$. Interpret your final model. 

```{r q3_data, echo = FALSE, include = FALSE}
carprice <- read_csv("car_price_prediction.csv") %>%
  mutate(fuelCNG = ifelse(fuel == 'CNG', 1,0)) %>%
  mutate(fuelDiesel = ifelse(fuel == 'Diesel', 1,0)) %>%
  mutate(fuelElectric = ifelse(fuel == 'Electric', 1,0)) %>%
  mutate(fuelLPG = ifelse(fuel == 'LPG', 1,0)) %>%
  mutate(fuelPetrol = ifelse(fuel == 'Petrol', 1,0)) %>%
  mutate(sellerDealer = ifelse(seller_type == 'Dealer',1,0)) %>%
  mutate(sellerIndividual = ifelse(seller_type == 'Individual',1,0)) %>%
  mutate(sellerTrustmark = ifelse(seller_type == 'Trustmark Dealer',1,0)) %>%
  mutate(transmissionAuto = ifelse(transmission == 'Automatic', 1,0)) %>%
  mutate(transmissionManual = ifelse(transmission == 'Manual', 1,0)) %>%
  mutate(firstOwner = ifelse(owner == 'First Owner', 1,0)) %>%
  mutate(secondOwner = ifelse(owner == 'Second Owner', 1,0)) %>%
  mutate(thirdOwner = ifelse(owner == 'Third Owner', 1,0)) %>%
  mutate(fourthOrMoreOwner = ifelse(owner == 'Fourth & Above Owner', 1,0)) %>%
  mutate(testDriveOwner = ifelse(owner == 'Test Drive Car', 1,0)) 
```

Let us perform some exploratory data analysis to see what kind of information our data contains. First we will examine the distribution of levels inside our factor varialbes, *fuel*, *transmission*,*seller_type*, and *owner*.

```{r q3_eda}
carprice %>%
  ggplot(aes(x = fuel, fill = as.factor(fuel))) + 
  geom_bar() + 
  xlab("Fuel Type") +
  ylab("Count")

carprice %>%
  ggplot(aes(x = transmission, fill = as.factor(transmission))) + 
  geom_bar() + 
  xlab("Transmission") +
  ylab("Count")

carprice %>%
  ggplot(aes(x = seller_type, fill = as.factor(seller_type))) + 
  geom_bar() + 
  xlab("Seller Type") +
  ylab("Count")

carprice %>%
  ggplot(aes(x = owner, fill = as.factor(owner))) + 
  geom_bar() + 
  xlab("Owner") +
  ylab("Count")
```

From these plots we see that most of our observations use Diesel or Petrol fuel, are manual transmission, sold by individuals, and are currently owned by the first owner. We will convert each of these factor variables into dummy variables 

```{r q3_rm, echo = FALSE}
carprice <- carprice %>% select(-c(fuel,seller_type,transmission,owner))
```

Next, we will see view how our continuous variables (*year* and *km_driven*) are correlated to our *y* variable, *selling_price*. 

```{r q3_panels}
pairs.panels(carprice[,c(2,4,3)])
```

We see from the plot above that *year* appears to have a positive, exponential effect on *selling_price* (bottom left plot), and *km_driven* has a negative, exponential effect on *selling_price* (bottom middle plot). The lines in red are a lowess smoother, not an exponential line. 

Now, we move forward to creating our Elastic Net model. I have chosen to stay with a LASSO regression since I like the feature of coefficients reducing to zero. First, we will split our data into training and testing data. For reproducibility, we will use a seed of 8561.

```{r q3_split}
# Split Data
set.seed(8561)
test_ind <- sample(nrow(carprice), 0.3*nrow(carprice))
car_test <- carprice[test_ind,]
car_train <- carprice[-test_ind,]
```

We will now run a series of cross-validated LASSO regression models using our training data to predict *selling_price* using all other variables (except the identifying factor, *name*).

```{r q3_model}
# Run Model
car_lasso_cv <- cv.glmnet(as.matrix(car_train[,-c(1,3)]), car_train$selling_price, alpha = 1)

# Examine model
plot(car_lasso_cv)
(car.lambda <- car_lasso_cv$lambda.min)
```

The plot above shows us that as $\lambda$ increases (even by a significant amount), our prediction accuracy measured in Mean-Squared Error (MSE) does not change very much. While the model shows that the $\lambda$ value that produces the smallest MSE is 807.6 (log($\lambda$) = 6.7), a $\lambda$ value of 22026 (log($\lambda$) = 10) has a comparable MSE value. Moving forward, we will use $\lambda$ = exp(10) = 22,026 to show us which of the variables are most important in predicting the price of a car. Below is a table of our non-zero coefficients for a LASSO model with $\lambda = e^{10}$. 

```{r q3_coef}
# View coeff at different lambdas
X <- predict(car_lasso_cv,s=exp(10),type="coefficients")
tibble(Variable = names(X[,1]), Coefficient = X[,1]) %>% filter(Coefficient != 0)
```

We see from this table that *year*,*km_driven*,*fuelDiesel*,*sellerIndividual*,*sellerTrustmark*,*transmissionAuto*,*transmissionManual*, and *firstOwner* are the most important for prediction. This is an interesting result for the following reasons:

1) The dummy variable for Diesel is the only one that remains. By our EDA results, most observations were either Diesel or Petrol cars. This remaining variable acts like an indicator for a diesel car. If a car uses diesel fuel it is predicted to be more expensive then any other car (mostly petrol). 

2) A similar thing happens with the dummy variables for *Owner*. Only the *firstOwner* variable remains and acts as an indicator for a first owner car or not. Our model is showing us in this data set that there is not a significant difference in the *selling_price* between second, third, and more than fourth owner cars. 

3) I am surprised that both *transmission* dummy variables remain in the final model. Because the factor is binary, I was for sure that at least one fo them would disappear. The price difference in automatic and manual cars must be different enough that our model decided to keep both dummy variables. 

4) The variables *year* and *km_driven* behave as we would expect from our EDA. 


Now let us see how our model(s) does at predicting unseen data. We will be using Root Mean Squared Error as a measurement of accuracy. The table below shows baseline RMSE (predict all *selling_price* values as the mean of *selling_price*), and RMSE using LASSO models with $log(\lambda) = 6.7$ (picked by glmnet) and $log(\lambda) = 10$ (picked by us).

```{r q3_pred}
# Check test accuracy
baseline_rmse <- sqrt(mean((mean(car_test$selling_price)-car_test$selling_price)^2))

car_preds_1 <- predict(car_lasso_cv, s = car.lambda, newx = as.matrix(car_test[,-c(1,3)]))
test_rmse_1 <- sqrt(mean((car_preds_1 - car_test$selling_price)^2))

car_preds_2 <- predict(car_lasso_cv, s = exp(10), newx = as.matrix(car_test[,-c(1,3)]))
test_rmse_2 <- sqrt(mean((car_preds_2 - car_test$selling_price)^2))

tibble(Model = c("Baseline", "LASSO (log(\u03BB) = 6.7)", "LASSO (log(\u03BB) = 10)"),
       Test_RMSE = c(baseline_rmse, test_rmse_1, test_rmse_2))
```

We see that both models are significantly better at prediction than the baseline, and using $log(\lambda) = 10$ (less variables) has a better RMSE than what glment picked. The RMSE tells us, that on average the prediction for the price of the car is off by about $424,000. Taking a quick look at the standard deviation of our test data:

```{r q3_sd}
sd(car_test$selling_price)
```

We see that our RMSE is smaller than the standard deviation, which is a good sign that we have strong predictive power. 

Our model coefficients for our LASSO model are harder to interpret because they are penalized. Let us run a standard least squares regression line using the variables picked by our LASSO model to get interpretable coefficients (we exclude *transmissionManual* because it is perfectly correlated with *transmissionAuto*). 

```{r q3_lm}
car_lm <- lm(selling_price~year+km_driven+fuelDiesel+sellerIndividual+sellerTrustmark+
               transmissionAuto+firstOwner,data = carprice)
summary(car_lm)
```

We can now interpret each of these variables as an increase or decrease in *selling_price* for a one unit increase in each of the variables. For example, for every additional km driven on the car, the price of the car decreases by $0.97. Overall, these variables explain about 45% of the variation in *selling_price*. 

And it appears I made an error. Even though our LASSO coefficients are penalized, they are all of the same magnitude (e+N) as the standard least squares line model. This means they will have a very similar interpretation as is, and computing our linear model was redundant (other than the fact that it taught me something new). 
